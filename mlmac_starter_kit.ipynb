{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlmac_starter_kit.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJeZTaeBv4HBIgJ7OLS7jt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitre-atlas/mlmac-starter-kit/blob/main/mlmac_starter_kit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLMAC: ML Model Attribution Challenge\n",
        "\n",
        "This notebook will get you up and running with the fine-tuned model API and the as well as the base models.\n",
        "\n",
        "Challenge details available at [mlmac.io](https://mlmac.io).\n",
        "\n",
        "See the official terms of service at [mlmac.io/terms](https://mlmac.io/terms)."
      ],
      "metadata": {
        "id": "nF1LEVUO8PYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "_tSr33hXc5uW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Dependencies"
      ],
      "metadata": {
        "id": "BH-0h-OpaqDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers > /dev/null\n",
        "!pip install sentencepiece > /dev/null"
      ],
      "metadata": {
        "id": "Pb1KIG0jam5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and import dependencies"
      ],
      "metadata": {
        "id": "Ku4-hH7qa-jb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import requests\n",
        "import time\n",
        "\n",
        "from pprint import pprint\n",
        "from transformers import pipeline\n",
        "from transformers.pipelines.conversational import Conversation"
      ],
      "metadata": {
        "id": "Re6uimg5auww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model API Helper\n",
        "\n",
        "Let's setup a helper class for interacting with the remote models. This let's us interact with the them in a more natural way and handles some common errors. It will also cache your queries (more on that later).\n",
        "\n",
        "Example usage:\n",
        "\n",
        "```\n",
        "input = \"The machine learning model attribution challenge is\"\n",
        "ft_model = Model(\"mlmac\", MLMAC_API_TOKEN, 0)\n",
        "output = ft_model(input)\n",
        "```\n",
        "\n",
        "```\n",
        "{'status': 'failed', 'result': {'error': 'currently loading', 'estimated_time': 125.36245727539062}, 'queries': {'0': 15, '1': 8, '4': 3, '10': 4}}\n",
        "attempt 1/10; waiting for 20 seconds\n",
        "{'status': 'failed', 'result': {'error': 'currently loading', 'estimated_time': 125.36245727539062}, 'queries': {'0': 15, '1': 8, '4': 3, '10': 4}}\n",
        "attempt 2/10; waiting for 20 seconds\n",
        "{'status': 'failed', 'result': {'error': 'currently loading', 'estimated_time': 125.36245727539062}, 'queries': {'0': 15, '1': 8, '4': 3, '10': 4}}\n",
        "attempt 3/10; waiting for 20 seconds\n",
        "{'generated_text': 'The machine learning model attribution challenge is a good '\n",
        "                   'start.'}\n",
        "```"
      ],
      "metadata": {
        "id": "JCncxHbNN_3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "  def __init__(self, api, api_token, model_id, use_cache=True):\n",
        "    self.api = api\n",
        "    self.api_token = api_token\n",
        "    self.model_id = model_id\n",
        "    self.use_cache = use_cache\n",
        "    self.cache = {}\n",
        "\n",
        "    if api == \"hf\":\n",
        "      self.api_url = f\"https://api-inference.huggingface.co/models/model-attribution-challenge/{model_id}\"\n",
        "    elif api == \"mlmac\":\n",
        "      self.api_url = f\"https://api.mlmac.io:8080/query?model={model_id}\"\n",
        "\n",
        "  def __call__(self, input, max_retries=10, params={}, options={}):\n",
        "    if self.use_cache and input in self.cache:\n",
        "      return self.cache[input]\n",
        "    \n",
        "    if self.api == \"hf\":\n",
        "      payload = {\"inputs\": input, \"parameters\": params, \"options\": options}\n",
        "    elif self.api == \"mlmac\":\n",
        "      payload = {\"input\": input}\n",
        "\n",
        "    headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
        "\n",
        "    for retry in range(max_retries):\n",
        "      response = requests.post(self.api_url, json=payload, headers=headers)\n",
        "    \n",
        "      if response.status_code == 200:\n",
        "        if self.api == \"hf\":\n",
        "          result = response.json()\n",
        "        elif self.api == \"mlmac\":\n",
        "          result = response.json().get(\"result\")\n",
        "\n",
        "        self.cache[input] = result\n",
        "\n",
        "        return result\n",
        "      elif response.status_code == 503:\n",
        "        print(response.json())\n",
        "        print(f\"attempt {retry+1}/{max_retries}; waiting for 20 seconds\")\n",
        "        time.sleep(20.0)\n",
        "      else: # error\n",
        "        raise Exception(response.text)\n",
        "    \n",
        "    raise Exception(f\"Failed after {max_retries} attempts\")"
      ],
      "metadata": {
        "id": "11E4lNW-FX-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-Tuned Models Setup\n",
        "\n",
        "You will interact with the fine-tuned models via the mlmac.io API."
      ],
      "metadata": {
        "id": "PSCDCKHLQyHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLMAC API Setup\n",
        "\n",
        "Retrieve your API token [here](https://api.mlmac.io:8080/github/auth), enter it in the code block below, and run the code block."
      ],
      "metadata": {
        "id": "-IdWt27bLt05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLMAC_API_TOKEN = \"\""
      ],
      "metadata": {
        "id": "HVkq4dXkhimH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To verify your API token is working, let's hit the `status` endpoint. \n",
        "The `status` endpoint is useful to check your total number of queries. Requests to this endpoint do not count as queries. You can also see your status at [mlmac.io/status](https://mlmac.io/status).\n",
        "\n",
        "Run the code block below. It will create a `status` helper function and execute it. You should see something like this:\n",
        "```\n",
        "{'api_key': 'your_api_key',\n",
        " 'created': '2022-07-14 20:47:41.339519',\n",
        " 'name': 'your_github_username',\n",
        " 'queries': {'0': 15, '1': 8, '10': 4, '4': 3},\n",
        " 'total_queries': 30}\n",
        "```"
      ],
      "metadata": {
        "id": "FNaRnoV1hoef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def status(api_token):\n",
        "  response = requests.get(f\"https://api.mlmac.io:8080/status\", headers={\"Authorization\": f\"Bearer {api_token}\"})\n",
        "  return response.json()\n",
        "\n",
        "status(MLMAC_API_TOKEN)"
      ],
      "metadata": {
        "id": "UPorOpfHLlXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuned Models\n",
        "\n",
        "You can go ahead and instantiate a class for each fine-tuned model for easy access later."
      ],
      "metadata": {
        "id": "elvLtjCeeJaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_models = [Model(\"mlmac\", MLMAC_API_TOKEN, idx) for idx in range(12)]"
      ],
      "metadata": {
        "id": "LpfOCHGCOM3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And try it out (note this will use a query). You should see something like:\n",
        "```\n",
        "{'generated_text': 'The machine learning model attribution challenge is a good '\n",
        "                   'start.'}\n",
        "```"
      ],
      "metadata": {
        "id": "yR3ezoAheYfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"The machine learning model attribution challenge is\"\n",
        "output = ft_models[0](input)\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "-WEC5GGwPQo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Query Caching\n",
        "\n",
        "The `Model` class stores (query, response pairs) in the `cache` member variable. This helps you avoid making extra non-useful queries."
      ],
      "metadata": {
        "id": "6Du7uvo7gqRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"total_queries\", status(MLMAC_API_TOKEN)[\"total_queries\"])\n",
        "pprint(ft_models[0](input))\n",
        "print(\"total_queries\", status(MLMAC_API_TOKEN)[\"total_queries\"])\n",
        "print(\"cache:\")\n",
        "pprint(ft_models[0].cache)"
      ],
      "metadata": {
        "id": "IFdjku0wg8pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving and Restoring Query Responses\n",
        "\n",
        "You may want to save your query/response pairs"
      ],
      "metadata": {
        "id": "tevGorT2X11w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./mlmac_ft_models.pkl\", \"wb\") as f:\n",
        "  pickle.dump(ft_models, f)"
      ],
      "metadata": {
        "id": "hd06VgjlYKMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The file can be downloaded (and later re-uploaded) via the Files menu in the colab sidebar.\n",
        "You can restore your saved models like this:"
      ],
      "metadata": {
        "id": "UdkLAAc8cDWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./mlmac_ft_models.pkl\", \"rb\") as f:\n",
        "  ft_models = pickle.load(f)"
      ],
      "metadata": {
        "id": "-a1LHO_HbG8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that you can see your cached responses:"
      ],
      "metadata": {
        "id": "Bc-9Zh45dhE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ft_models[0].cache"
      ],
      "metadata": {
        "id": "ik_YcRUAbSlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you like you can mount your drive and save your pkl there for easy storage and retrievel, but [be careful who you colab with](https://medium.com/mlearning-ai/careful-who-you-colab-with-fa8001f933e7). ðŸ˜ˆ"
      ],
      "metadata": {
        "id": "17fiLv3uYMuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "F2uF_jiOX34w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/mlmac_ft_models.pkl\", \"wb\") as f:\n",
        "  pickle.dump(ft_models, f)"
      ],
      "metadata": {
        "id": "Xyo7T_vudwU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/mlmac_ft_models.pkl\", \"rb\") as f:\n",
        "  ft_models = pickle.load(f)"
      ],
      "metadata": {
        "id": "7mCGLL9Fd1S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Models Setup\n",
        "\n",
        "We can interact with the base models in two ways: via the Hugging Face API (similar to MLMAC), or directly via their python interface."
      ],
      "metadata": {
        "id": "LJITVnjWXZXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hugging Face API Setup\n",
        "\n",
        "Similar to the setup for the MLMAC API, we set our API Token and use our helper class to instantiate an interface to each base model.\n",
        "\n",
        "You can create an API token [here](https://huggingface.co/settings/tokens). You'll need a Hugging Face account."
      ],
      "metadata": {
        "id": "qPMGz2W3VISw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HF_API_TOKEN = \"\"\n",
        "\n",
        "base_model_names = [\"bloom-350m\", \"bloom-2b5\", \"codegen-350M-multi\", \"DialoGPT-large\",\n",
        "                    \"distilgpt2\", \"gpt2\", \"gpt2-xl\", \"gpt-j-6B\", \"gpt-neo-125M\",\n",
        "                    \"Multilingual-MiniLM-L12-H384\", \"opt-350m\", \"xlnet-base-cased\"]\n",
        "\n",
        "base_models = {model_name: Model(\"hf\", HF_API_TOKEN, model_name, use_cache=False) for model_name in base_model_names}"
      ],
      "metadata": {
        "id": "g3kuEnGSP1eh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out:"
      ],
      "metadata": {
        "id": "zFElBXQ-WKQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"The machine learning model attribution challenge is\"\n",
        "output = base_models[\"gpt2\"](input)\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "dMtE2N1U9bBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also pass parameters and options to huggingface as documented [here](\"https://huggingface.co/docs/api-inference/detailed_parameters\")."
      ],
      "metadata": {
        "id": "d16pzO_kcE--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = {\"do_sample\": True, \"num_return_sequences\": 3}\n",
        "options = {\"use_cache\": False}\n",
        "output = base_models[\"gpt2\"](input, params=params, options=options)\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "Jy4oTOmJcIx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run Base Models in Colab\n",
        "\n",
        "You may find it useful to have full access to and control over the base models. You can load the models directly using the transformers library: "
      ],
      "metadata": {
        "id": "o0dDwr7_aJRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input = \"The machine learning model attribution challenge is\"\n",
        "\n",
        "base_model = pipeline(\"text-generation\", model=\"model-attribution-challenge/gpt2\")\n",
        "output = base_model(input)\n",
        "\n",
        "pprint(output)"
      ],
      "metadata": {
        "id": "mzylPSnxmwDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLMAC Solution\n",
        "\n",
        "This is the part you do. Good luck!"
      ],
      "metadata": {
        "id": "Yi7fBVugZAba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "bwiO2_i7jvAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Solution Submission\n",
        "\n",
        "Submit your solution at [mlmac.io/submit](https://mlmac.io/submit/)."
      ],
      "metadata": {
        "id": "MbJGu1g8jywy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "my_solution = np.random.permutation(np.eye(13, dtype=np.uint8))[:12]\n",
        "\n",
        "cols = [\"BLOOM 350m\", \"BLOOM 2b5\", \"CodeGen-Multi 350m\", \"DialoGPT Large\",\n",
        "        \"DistilGPT-2\", \"GPT-2\",\"GPT-2 XL\", \"GPT-J 6b\", \"GPT-Neo 125m\",\n",
        "        \"Multilingual MiniLM L12 H384\", \"OPT 350m\", \"XLNet Base Cased\", \"None\"]\n",
        "solution = pd.DataFrame(my_solution, columns=cols) \n",
        "solution.index.name = \"Fine-Tuned Model\"\n",
        "solution = solution.transpose().idxmax()\n",
        "solution"
      ],
      "metadata": {
        "id": "01D-mMoLmWFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the submission URL from your solution:"
      ],
      "metadata": {
        "id": "7w8i7bTlj_ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "form_url = \"https://docs.google.com/forms/d/e/1FAIpQLSferAdg6VAQVVeXVdSJHKT0e2Kp5VB_jDirR2hvP_m_xrkSkw/viewform\"\n",
        "fields = [\"entry.1315922193\", \"entry.1345806853\", \"entry.67790734\", \"entry.1474144619\",\n",
        "          \"entry.784500124\", \"entry.661323761\", \"entry.1507213413\", \"entry.524188892\",\n",
        "          \"entry.1715278878\", \"entry.1437304796\", \"entry.192234817\", \"entry.552389875\"]\n",
        "soln_url = f\"{form_url}?entry.403209797={MLMAC_API_TOKEN}&\" + \"&\".join([f\"{field}={model.replace(' ', '+')}\" for field, model in zip(fields, solution)])\n",
        "print(soln_url)"
      ],
      "metadata": {
        "id": "tKTqqu3Qa-D5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BnDrZ3KJmFub"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}